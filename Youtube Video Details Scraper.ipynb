{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from scrapy.selector import Selector\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "    Function to obtain the data and store it in a Pandas DataFrame format\n",
    "    Inputs:\n",
    "    No Inputs\n",
    "    Return Values:\n",
    "    The function returns a pandas DataFrame with only one column by the name of 'video link'.\n",
    "    '''\n",
    "    ## Reading a csv with links of random youtube videos \n",
    "    df=pd.read_csv('Youtube_links.csv')\n",
    "    \n",
    "    ## Formatting the data frame\n",
    "    df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    df.columns=[ 'video link']\n",
    "    \n",
    "    ## Taking a look at the dataframe\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_crawler(youtube_data):\n",
    "    '''\n",
    "    This fuction is used to scrape the required content from each of the Youtube URLs and return\n",
    "    the data in an appropriate format.\n",
    "    Inputs:\n",
    "    youtube_data: takes a dataframe with one column by the name of 'video link' which contains the video urls.\n",
    "    Return Values:\n",
    "    The function returns a dictonary with all the extracted information, which can then be exported to a csv format using Pandas\n",
    "    '''\n",
    "    \n",
    "    ##initializing the lists which will be then used to store the data we have scraped from \n",
    "    \n",
    "    dates=[]\n",
    "    comments=[]\n",
    "    likes=[]\n",
    "    dislikes=[]\n",
    "    views=[]\n",
    "    links=[]\n",
    "    count=0\n",
    "    try:\n",
    "        \n",
    "        ## The following for loop iterates through all the urls to scrape the required information\n",
    "        for link in youtube_data['video link']:\n",
    "\n",
    "            ##The scraper uses Selenium Webdriver to get the information, Below is the code to add options for the webdriver.\n",
    "            options = Options()\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.page_load_strategy = 'eager'\n",
    "\n",
    "            ##initializing the webdriver\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "\n",
    "            ##Opening the url with the help of the webdriver\n",
    "            driver.get(link)\n",
    "\n",
    "            ## A delay is added to give time for the webpage to load \n",
    "            time.sleep(2)\n",
    "\n",
    "            ## Initializing a scrapy selector, which helps us in finding the elements required\n",
    "            sel=Selector(text=driver.page_source)\n",
    "\n",
    "            ##Scrolling down to the portion of the webpage where the number of contents have been mentioned\n",
    "            driver.execute_script('window.scrollTo(1, 500);')\n",
    "\n",
    "            #now we wait let load the comments\n",
    "            time.sleep(5)\n",
    "\n",
    "            ## The following chunk of code finds out the number of comments. If the number is not present, it implies that \n",
    "            #  the comments have been turned off for the video.\n",
    "            try:\n",
    "                comment_num=driver.find_element_by_xpath(\"//h2[@id='count']\").text[:-9]\n",
    "            except:\n",
    "                comment_num=\"Comments are turned off\"\n",
    "\n",
    "            ## The following line of code helps us in identifying the date of upload\n",
    "            date=sel.xpath('//*[@id=\"date\"]/yt-formatted-string/text()').get()\n",
    "\n",
    "            ##  The following chunk of code extracts the number of likes and dislikes. If an element for the number of likes/dislikes is\n",
    "            #   absent, it signifies that there are no likes/dislikes for the particular video \n",
    "            try:\n",
    "                like=sel.xpath('//*[@id=\"top-level-buttons\"]/ytd-toggle-button-renderer[1]/a/yt-formatted-string/@aria-label').get()[:-6]\n",
    "            except:\n",
    "                like=0\n",
    "            try:\n",
    "                dislike=sel.xpath('//*[@id=\"top-level-buttons\"]/ytd-toggle-button-renderer[2]/a/yt-formatted-string/@aria-label').get()[:-9]\n",
    "            except:\n",
    "                dislike=0\n",
    "\n",
    "            ## The next chunk of code identifies the number of views for a video and appends it to the respective list.\n",
    "            view=driver.find_element_by_xpath('//*[@id=\"count\"]/yt-view-count-renderer/span[1]').text[60:-12]\n",
    "\n",
    "            ## The driver is not required \n",
    "            driver.close()\n",
    "            dates.append(date)\n",
    "            comments.append(comment_num)\n",
    "            likes.append(like)\n",
    "            dislikes.append(dislike)\n",
    "            views.append(view)\n",
    "            links.append(link)\n",
    "            count=count+1\n",
    "    \n",
    "    ##  The try except block is to ensure that if the process gets interrupted due to a faulty internet connection, then the process is \n",
    "    #   resumed from where it got left off. If the webpage does not load then the webdriver is not able to find the required elements on\n",
    "    #   the webpage\n",
    "    #   This is essential because the process is lengthy as we are scraping data for 500 urls.\n",
    "    except Exception as e:\n",
    "        print(\"The number of files that have been downloaded are: \",count)\n",
    "        print(\"The process was interrupted. The error that occured was :\",e)\n",
    "        \n",
    "        ## We count how many files have been downloaded and the function calls itself to resume the process from the required start point\n",
    "        if(count!=len(youtube_data['video link'])):\n",
    "            data_crawler(youtube_data.loc[count:])\n",
    "    \n",
    "    return {'video link':links,\n",
    "            'video views':views,\n",
    "            'uploaded date':dates,  \n",
    "            'comments':comments,\n",
    "            'likes':likes,\n",
    "            'dislikes':dislikes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_crawler(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    ## Running the get_data function to load the csv\n",
    "    youtube_data=get_data\n",
    "    \n",
    "    ## Calling the data_crawler function to extract the required information from the urls\n",
    "    data=data_crawler(youtube_data)\n",
    "    \n",
    "    ##Since the data that has been obtained is in the form of a dictonary its converted to a data frame\n",
    "    data_df=pd.DataFrame(data)\n",
    "    \n",
    "    ##Now the data is exported in the form of a csv \n",
    "    data_df.to_csv('Youtube_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
